{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_lab_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKZ4sMG3LAmL"
      },
      "source": [
        "# Deep learning with Tensorflow - Lab exercise\n",
        "The purpose of this lab is to give a short demonstration of how to fit CNNs to image data. The example was constructed for this demonstration and is not realistic in some aspects. This has the purpose that the model fitting converges in a reasonable amount of time to be feasible for this lab. The example is further limited through the computational resources that are provided in Google Colab. While the content of the example may not be completely realistic, the procedure would be identical for a realistic application.\n",
        "\n",
        "The images that we are using were derived from whole-slide-images (WSIs) of resected prostate tumors that are publicily available through The Cancer Genome Archive (TCGA). Each WSI in the TCGA PRAD (prostate adenoma and adenocarcinoma) dataset was split into smaller image patches. Using a cancer detection CNN, we then selected image patches that likely contain malignant cells.\n",
        "\n",
        "The task that we focus on here is the classification of image patches of prostate cancer into \"low\" and \"high\" grade. The low grade class is constructed from image patches of patient with a cancer with ISUP grade 1 whereas the high grade image patches originate from patients with ISUP grade 5.\n",
        "\n",
        "#### Important Note: Make sure to use the GPUs instead of CPUs (Why?). Go to Edit > Notebook settings > choose GPU if not selected already"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRSsbq7sLAmO"
      },
      "source": [
        "## 1. Imports, convenience functions and preparation of dataset\n",
        "In the following section, we will first import the packages that we need for the further code. Then, we will define some convenience functions for plotting results and download and unzip the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfIbY4YpPdVt"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYS-ACRwLAmW"
      },
      "source": [
        "Now that we imported the appropriate packages, we define two convenience functions to plot results. `plot_history` will show the learning curves of the model fitting process. `plot_roc` will show the receiver operating characteristic of the predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2nR65DaLAmX"
      },
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()),1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(dpi=150)\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRLZVAKuLAma"
      },
      "source": [
        "The following code section will first download the dataset that we prepared for this task and then unzip it. The download process has a progress bar, but the unzipping unfortunately does not, so please be patient for a while after the download finished. The total time of the next cell should not be more than 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNR1f6zZS5yB"
      },
      "source": [
        "!wget -O /content/data_lab.zip https://www.dropbox.com/scl/fi/meovuq3l3lkrhf2z0mq61/data_lab.zip?rlkey=4t1mec88ih21pyti83m5w2931&dl=0\n",
        "print('Unzipping folder... this may take a few minutes without output. Be patient.')\n",
        "with zipfile.ZipFile('data_lab.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_lab')\n",
        "\n",
        "path_data = '/content/data_lab/data_lab'\n",
        "train_dir = os.path.join(path_data, 'train')\n",
        "validation_dir = os.path.join(path_data, 'valid')\n",
        "test_dir = os.path.join(path_data, 'test')\n",
        "print('Directories in unzipped folder:', os.listdir(path_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmAuyeOrLAme"
      },
      "source": [
        "## 2. Dataset definition and data augmentation\n",
        "In this section, we will define a training, validation and test dataset and show an example of data augmentation. There are many different ways in tensorflow to set up data sets. In this case, a dataset is initialised through providing a path to a directory that contains a folder named \"low\" and one named \"high\" that contain images of the respective classes. We already split all image patches into training, validation and test folder, so we do not need to sample different folds at this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uwox8-FPnSr"
      },
      "source": [
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=32,\n",
        "                                             image_size=(598, 598))\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=32,\n",
        "                                                  image_size=(598, 598))\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=32,\n",
        "                                            image_size=(598, 598))\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puaFeAdcLAmi"
      },
      "source": [
        "Now that we defined our datasets, we would like to visually confirm that everything is in order. To this end, we will plot the first 9 images of the first batch with their respective labels. Note that the labels \"high\" and \"low\" were assigned to the entire tumor. However, high-grade tumors can sometimes contain some lower grade regions. Unfortunately, we only have the grade on a patient level, so the \"high\" grade class display a few low-grade examples as well. While this is not ideal to fit CNNs, the CNN will still be able to learn to classify most images correctly but this gives an upper bound to the highest performance that we can expect the model to achieve. You can run the next cell multiple times to see a larger variety of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWS3DglVPwp4"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfDIGrfOLAmm"
      },
      "source": [
        "A common trick to increase the number of different images that a CNN is trained with is to use data augmentation. In this case, we will randomly mirror and rotate the image patches. Since the filters that a CNN learns are not rotation-invariant, rotated and mirrored image patches will seem like \"new\" images to the model to a certain extent. Note that rotating images only is appropriate if the images are in fact rotation-invariant. If the rotation of the image matters to the meaning of the image, e.g. when showing a skyline of buildings, it may depend on the specific circumstances whether it is a good idea to rotate them. However, for histopathology images, the rotation at which one examines a glass slide should not matter. Therefore, it is a good idea to use this type of data augmentation here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUcHDI0jQL6K"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyTCUVmHLAmq"
      },
      "source": [
        "Now that we have defined our data augmentation functions, let's take a look at what they do to an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09mMAjRsQNsW"
      },
      "source": [
        "for image, _ in train_dataset.take(1):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  first_image = image[0]\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "    plt.imshow(augmented_image[0] / 255)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXJiR1cWLAmu"
      },
      "source": [
        "## 3. Model definition\n",
        "In this section, we will define the model that we would like to fit. We will deploy transfer learning, as was described in the lecture. We will therefore define MibileNetV2 with ImageNet weights as the base model and add a couple of necessary layers to it. Specifically, we add a pooling layer to reduce the final feature maps to a vector, a drop-out layer to reduce overfitting and a fully connected (in tensorflow referred to as \"dense\") layer with a sigmoid activation function to receive an output that can be interpreted as a probability (although it amy not be well calibrated)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjO6wDwQRYLl"
      },
      "source": [
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mLOGjpLAmx"
      },
      "source": [
        "In this example, we would like to only fit the last 20% of model layers to our images whereas the lower layers of the model will remain fixed. On the one hand, this reduces GPU memory requirements.\n",
        "\n",
        "Can you think of further reasons of why we would like to do this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQEvRUy5Q_OZ"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS7NZcPNLAm1"
      },
      "source": [
        "Now we need to define a learning rate, a loss function and an optimizer and then compile the model before training. We choose a very low value for the learning rate, which is typical in transfer learning. The learning rate scales how much the gradient of the difference between prediction and loss changes the model weights. Since our weights are probably already a good starting point from their ImageNet training, we do not want to change them strongly throughout model fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQ8GNr_RBJW"
      },
      "source": [
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YPMin0dLAm4"
      },
      "source": [
        "## 4. Model fitting\n",
        "Now we are all set to fit the model to our training data. Since this problem is constructed to be very easy, we will only use 25 batches per epoch to fit the model, which is unrealistically small for a real-world application. To get a more reliable estimate of the model performance, we will evaluate on 50 batches. Typically, the number of validation steps is chosen to be a lot smaller than the number of training steps to not use too much computation time on validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWC70lSSTyHj"
      },
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=50,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zokXB-jVLAm9"
      },
      "source": [
        "## 5. Visualisation of results\n",
        "Now that we fitted a model, we would like to inspect the learning curves on the training and validation data. For this relatively easy task, they should be quite similar. However, for more difficult tasks and small datasets, the learning curves could of course exhibit a typical pattern for over- or underfitting.\n",
        "\n",
        "For most of you, the training accuracy should be lower than the validation accuracy throughout the training (this may vary due to random chance). Do you have an idea why the validation performance could be higher?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6R9KCOzULlX"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXJRRB8yLAnA"
      },
      "source": [
        "Since the learning rates appear reasonably plausible, we will would like to predict the class of a couple of tiles from the test set and compare with their true label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbxPquDOXNNG"
      },
      "source": [
        "#Retrieve a batch of images from the test set\n",
        "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
        "predictions = model.predict_on_batch(image_batch).flatten()\n",
        "predictions = tf.where(predictions < 0.5, 0, 1)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
        "  plt.title('True: {}, Predicted: {}'.format(class_names[predictions[i]], class_names[label_batch[i]]))\n",
        "  plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLsBKXjyLAnD"
      },
      "source": [
        "Finally, let's take a look at the ROC of the model that we fitted. For this, we will predict all approx. 120 test set batches, which will take 1-2 minutes.\n",
        "\n",
        "Do you think that this is a good model performance for this task? Please motivate your answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKYC_IdfLAnE"
      },
      "source": [
        "predictions = list()\n",
        "labels = list()\n",
        "for image_batch, label_batch in tqdm(test_dataset.as_numpy_iterator()):\n",
        "    predictions_batch = model.predict_on_batch(image_batch).flatten()\n",
        "    predictions.append(predictions_batch)\n",
        "    labels.append(label_batch)\n",
        "y_score = np.concatenate(predictions)\n",
        "y_test = np.concatenate(labels)\n",
        "\n",
        "plot_roc(y_test, y_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-HFusNwLAnH"
      },
      "source": [
        "And we are done! This is everything you that you need to do to fit a CNN to some image data! If you would like to play around with this example, e.g. see what happens when you change the learning rate or set `weights=None` so that the MobileNetV2 was not pretrained on ImageNet, you need to restart this notebook and download and unzip the data again since unfortunately, there is no easy way to release the memory that the model currently uses."
      ]
    }
  ]
}
