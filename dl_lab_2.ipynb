{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QOV1EPP0dygG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()),1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(dpi=150)\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FgzzApded2A_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and preparing the dataset\n",
        "!wget --load-cookies /tmp/data_lab.zip \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/data_lab.zip --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1\" -O data_lab.zip && rm -rf /tmp/data_lab.zip\n",
        "\n",
        "print('Unzipping folder... this may take a few minutes without output. Be patient.')\n",
        "with zipfile.ZipFile('data_lab.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_lab')\n",
        "\n",
        "path_data = '/content/data_lab/data_lab'\n",
        "train_dir = os.path.join(path_data, 'train')\n",
        "validation_dir = os.path.join(path_data, 'valid')\n",
        "test_dir = os.path.join(path_data, 'test')\n",
        "print('Directories in unzipped folder:', os.listdir(path_data))\n",
        "\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=32,\n",
        "                                             image_size=(598, 598))\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=32,\n",
        "                                                  image_size=(598, 598))\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=32,\n",
        "                                            image_size=(598, 598))\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGsD59-hq9ye",
        "outputId": "7066e3fb-2d4e-4cf5-b288-be11cfa72ace"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-15 12:27:13--  https://docs.google.com/uc?export=download&confirm=t&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.142.138, 74.125.142.113, 74.125.142.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.142.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-48-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e1dijljev2craqqtp5ndpqt5c3mfd2h8/1700051175000/13525664275705142765/*/1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1?e=download&uuid=a6406d48-8f61-4d4d-bc95-e3d80a248f52 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-11-15 12:27:13--  https://doc-04-48-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e1dijljev2craqqtp5ndpqt5c3mfd2h8/1700051175000/13525664275705142765/*/1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1?e=download&uuid=a6406d48-8f61-4d4d-bc95-e3d80a248f52\n",
            "Resolving doc-04-48-docs.googleusercontent.com (doc-04-48-docs.googleusercontent.com)... 173.194.203.132, 2607:f8b0:400e:c05::84\n",
            "Connecting to doc-04-48-docs.googleusercontent.com (doc-04-48-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3147933203 (2.9G) [application/zip]\n",
            "Saving to: ‘data_lab.zip’\n",
            "\n",
            "data_lab.zip        100%[===================>]   2.93G  87.7MB/s    in 37s     \n",
            "\n",
            "2023-11-15 12:27:50 (82.0 MB/s) - ‘data_lab.zip’ saved [3147933203/3147933203]\n",
            "\n",
            "Unzipping folder... this may take a few minutes without output. Be patient.\n",
            "Directories in unzipped folder: ['test', 'df_tile_lab.pkl', 'train', 'valid']\n",
            "Found 10927 files belonging to 2 classes.\n",
            "Found 3678 files belonging to 2 classes.\n",
            "Found 3660 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class labels name\n",
        "print(class_names)\n",
        "## looking at the number of images from each class in the training data\n",
        "print('Number of samples with gleason grade high: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/high/'))))\n",
        "print('Number of samples with gleason grade low: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/low/'))))"
      ],
      "metadata": {
        "id": "VIh38k1y9S5R",
        "outputId": "bce6d242-c1ec-4e7a-c03a-f42a899b7b03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['high', 'low']\n",
            "Number of samples with gleason grade high: 97\n",
            "Number of samples with gleason grade low: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir /content/data_lab/data_lab/train_copy\n",
        "# !cp -r /content/data_lab/data_lab/train/ /content/data_lab/data_lab/train_copy/\n",
        "\n",
        "# grade_low_dirs = os.listdir('/content/data_lab/data_lab/train_copy/train/low')\n",
        "# # grade_low_dirs\n",
        "# for dir in grade_low_dirs[:18]:\n",
        "#   path_to_dir = os.path.join('/content/data_lab/data_lab/train_copy/train/low/', dir)\n",
        "#   # print(path_to_dir)\n",
        "\n",
        "## introducing class weights\n",
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n"
      ],
      "metadata": {
        "id": "8IzHEWtl9fA9",
        "outputId": "4f33cbf9-3f84-42f0-c99b-1da3c83f07d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for class 0: 1.78\n",
            "Weight for class 1: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining data augmentation, potentially can add more different ones\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "LIYXLXps2lEt",
        "outputId": "052d5c37-da87-4800-a48e-00c409d69aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  154\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 598, 598, 3)]     0         \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, 598, 598, 3)       0         \n",
            "                                                                 \n",
            " tf.math.truediv_2 (TFOpLam  (None, 598, 598, 3)       0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " tf.math.subtract_2 (TFOpLa  (None, 598, 598, 3)       0         \n",
            " mbda)                                                           \n",
            "                                                                 \n",
            " mobilenetv2_1.00_224 (Func  (None, None, None, 1280   2257984   \n",
            " tional)                     )                                   \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 1280)              0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1281      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2259265 (8.62 MB)\n",
            "Trainable params: 1619841 (6.18 MB)\n",
            "Non-trainable params: 639424 (2.44 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying class weights during model training\n",
        "neg = len(os.listdir('/content/data_lab/data_lab/train/low/'))\n",
        "pos = len(os.listdir('/content/data_lab/data_lab/train/high/'))\n",
        "total = neg + pos\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n"
      ],
      "metadata": {
        "id": "1M5T56dm9dnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=50,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_dataset,\n",
        "                    class_weight=class_weight)\n",
        "\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "eGP2XAJN2_wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# introducing callbacks and earlystopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=10,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_dataset,\n",
        "                    callbacks=[callback],\n",
        "                    verbose=1)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "NyBAoQis0fUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}