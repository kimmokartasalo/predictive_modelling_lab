{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QOV1EPP0dygG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()),1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(dpi=150)\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FgzzApded2A_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and preparing the dataset\n",
        "!wget --load-cookies /tmp/data_lab.zip \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/data_lab.zip --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1\" -O data_lab.zip && rm -rf /tmp/data_lab.zip\n",
        "\n",
        "print('Unzipping folder... this may take a few minutes without output. Be patient.')\n",
        "with zipfile.ZipFile('data_lab.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_lab')\n",
        "\n",
        "path_data = '/content/data_lab/data_lab'\n",
        "train_dir = os.path.join(path_data, 'train')\n",
        "validation_dir = os.path.join(path_data, 'valid')\n",
        "test_dir = os.path.join(path_data, 'test')\n",
        "print('Directories in unzipped folder:', os.listdir(path_data))\n",
        "\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=32,\n",
        "                                             image_size=(598, 598))\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=32,\n",
        "                                                  image_size=(598, 598))\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=32,\n",
        "                                            image_size=(598, 598))\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "qGsD59-hq9ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class labels name\n",
        "print(class_names)\n"
      ],
      "metadata": {
        "id": "VIh38k1y9S5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "tranfer_learning = False\n",
        "if tranfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "# x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "LIYXLXps2lEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=50,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "eGP2XAJN2_wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe the model did not learn anything with random initalisation of the weights. A lot of data is required to fine tune the millions of parameters (Note: here we only fine tune the last 20% of the layers of the network so we did not optimise much in the 80% of the network).\n",
        "\n",
        "Try running the model fitting with transfer learning where the pretrained model fitted on the Imagenet dataset is used to fine tune on the pathology images (same as the lab 1)"
      ],
      "metadata": {
        "id": "auG81SDMTKxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "# tranfer_learning = True or False\n",
        "\n",
        "if tranfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "# x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=50,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "supgagG3UXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will work on a strategy to deal with the class imbalance problem."
      ],
      "metadata": {
        "id": "2g0AW35_RO-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## looking at the number of images from each class in the training data\n",
        "print('Number of samples with gleason grade high: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/high/'))))\n",
        "print('Number of samples with gleason grade low: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/low/'))))\n",
        "# applying class weights during model training\n",
        "neg = len(os.listdir('/content/data_lab/data_lab/train/low/'))\n",
        "pos = len(os.listdir('/content/data_lab/data_lab/train/high/'))\n",
        "total = neg + pos\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class low: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class high: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "id": "mFpOlQXvPv5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will introduce class weights during the model training so the it takes into account the ratio of class imbalance while calculating the evaluation metric (i.e. accuracy) during the model training"
      ],
      "metadata": {
        "id": "fqoTpnuDRhi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=50,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_dataset,\n",
        "                    class_weight=class_weight)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "4lg44LOGPyvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What changes did you observe in the model learning curves?"
      ],
      "metadata": {
        "id": "xFV2Sw2lR9P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will observe if the model is overfitting and common strategies to deal with it.\n",
        "Strategies: including the data augmentation in the input, the dropout layer in the base model and the early stopping callback in the model training"
      ],
      "metadata": {
        "id": "g9Aj9d1kVpXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining data augmentation, potentially can add more different ones\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "tranfer_learning = False\n",
        "if tranfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "# x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "## include the dropout layer\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "# introducing callbacks and earlystopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=10,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_dataset,\n",
        "                    callbacks=[callback],\n",
        "                    verbose=1)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "yTeTAXQqSWV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If time allows you can apply the strategies one by one to see their effect on the model learning curves individually. You can try different parameters for dropout layer (dropout rate), early stopping (different evaluation metric to monitor, patience value etc.)"
      ],
      "metadata": {
        "id": "_lnaE9HIWC0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last part of the lab includes the optimisation of the hyperparameter: learning rate"
      ],
      "metadata": {
        "id": "FSWibd0IXPDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "NyBAoQis0fUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}