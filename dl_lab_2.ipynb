{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will go through the different aspects of common deep learning based problems, i.e. dealing with low dataset via transfer learning, class imbalance problem, overfitting and lastly hyperparameter optimisation"
      ],
      "metadata": {
        "id": "o7WmdloAg6km"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOV1EPP0dygG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()),1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(dpi=150)\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FgzzApded2A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and preparing the dataset\n",
        "\n",
        "!wget -O /content/data_lab.zip https://www.dropbox.com/scl/fi/meovuq3l3lkrhf2z0mq61/data_lab.zip?rlkey=4t1mec88ih21pyti83m5w2931&dl=0\n",
        "print('Unzipping folder... this may take a few minutes without output. Be patient.')\n",
        "with zipfile.ZipFile('data_lab.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_lab')\n",
        "\n",
        "path_data = '/content/data_lab/data_lab'\n",
        "train_dir = os.path.join(path_data, 'train')\n",
        "validation_dir = os.path.join(path_data, 'valid')\n",
        "test_dir = os.path.join(path_data, 'test')\n",
        "print('Directories in unzipped folder:', os.listdir(path_data))\n",
        "\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=16,\n",
        "                                             image_size=(598, 598))\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=16,\n",
        "                                                  image_size=(598, 598))\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=16,\n",
        "                                            image_size=(598, 598))\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "qGsD59-hq9ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class labels name\n",
        "print(class_names)\n"
      ],
      "metadata": {
        "id": "VIh38k1y9S5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will look at the impact of the transfer learning. We have introduced the option of either using the Imagenet pretrained model or using the model with randomly initialised weights.\n",
        "First we will train the model with the randomly initialised weights."
      ],
      "metadata": {
        "id": "Jkq1j-Tptp3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = False\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=25,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "LIYXLXps2lEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe the model did not learn anything with random initalisation of the weights. A lot of data is required to fine tune the millions of parameters (Note: here we only fine tune the last 20% of the layers of the network so we did not optimise anything in the 80% of the network).\n",
        "\n",
        "Try running the model fitting with transfer learning where the pretrained model fitted on the Imagenet dataset is used to fine tune on the pathology images (same as the lab 1)"
      ],
      "metadata": {
        "id": "auG81SDMTKxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = True\n",
        "\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=25,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "supgagG3UXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What changes did you observe in the model learning curves?"
      ],
      "metadata": {
        "id": "xFV2Sw2lR9P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will include and observe the model overfitting and common strategies to deal with it.\n",
        "Strategies: including the data augmentation in the input, the dropout layer in the base model and the early stopping callback in the model training.\n",
        "\n",
        "Here we reduce the training set to make the model overfit quickly"
      ],
      "metadata": {
        "id": "g9Aj9d1kVpXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reducing the training dataset by a huge margin to introduce the overfitting\n",
        "train_dataset_subset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=16,\n",
        "                                             image_size=(598, 598),\n",
        "                                             validation_split = 0.99,\n",
        "                                             seed = 523,\n",
        "                                             subset = 'training')\n",
        "train_dataset_subset = train_dataset_subset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()"
      ],
      "metadata": {
        "id": "UNKrUxkMP49m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train the model on the reduced training set"
      ],
      "metadata": {
        "id": "mQFOYy1TuyJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = True\n",
        "\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "# steps_per_epoch = len(list(train_dataset_subset))\n",
        "# print(len(list(train_dataset_subset)))\n",
        "steps_per_epoch = int(110/16)\n",
        "history = model.fit(train_dataset_subset,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=25,\n",
        "                    epochs=30,\n",
        "                    validation_data=validation_dataset)\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "_0do11Ytudoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can clearly observe the overfitting in the learning curves.\n",
        "Can you mention how do we observe the overfitting through learning curves?"
      ],
      "metadata": {
        "id": "ZcJbjI3Hu7wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing the strategies to deal with overfitting\n",
        "Here we have included the: data augmentation, drop-out layer and earlystopping callback.\n",
        "\n"
      ],
      "metadata": {
        "id": "FZj-gj5O5zsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining data augmentation, potentially can add more different ones\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = True\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "## include the dropout layer\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "# introducing callbacks and earlystopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "steps_per_epoch = int(110/16)\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=25,\n",
        "                    epochs=30,\n",
        "                    validation_data=validation_dataset,\n",
        "                    callbacks=[callback],\n",
        "                    verbose=1)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "yTeTAXQqSWV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can observe the training stopped early due to the earlystopping callback criteria which avoided the overfitting as we observed in the previous model learning curves.\n",
        "If time allows you can apply the strategies one by one to see their effect on the model learning curves individually. You can try different parameters for dropout layer (dropout rate), early stopping (different evaluation metric to monitor, patience value etc.)"
      ],
      "metadata": {
        "id": "_lnaE9HIWC0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last part of the lab includes the optimisation of the hyperparameter: learning rate. We use the grid search through a list of learning rates and observe the model learning curves."
      ],
      "metadata": {
        "id": "FSWibd0IXPDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## creating a grid of learning rates\n",
        "list_of_learning_rates = [0.000001, 0.00001, 0.0001]\n",
        "for learning_rate in list_of_learning_rates:\n",
        "\n",
        "  loss = tf.keras.losses.BinaryCrossentropy()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "  history = model.fit(train_dataset,\n",
        "                      steps_per_epoch=25,\n",
        "                      validation_steps=25,\n",
        "                      epochs=15,\n",
        "                      validation_data=validation_dataset,\n",
        "                      callbacks=[callback],\n",
        "                      verbose=1)\n",
        "  plot_history(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyBAoQis0fUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will have to wait for the three models to finish training. This might take few minutes.\n",
        "\n",
        "What do you think is the optimum choice for the learning based on the model learning curves?\n",
        "Also you can implement more dynamic learning rate schedulers as discussed in the lecture and observe their impact.\n",
        "\n",
        "Next part is an optional extra exercise where we implement a famous strategy to deal with class imbalance problem. We introduce the class weighting during the model training to account for the different proportion of classes during loss calculation in every epoch."
      ],
      "metadata": {
        "id": "bcI1hTQZj1dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## looking at the number of images from each class in the training data\n",
        "print('Number of samples with gleason grade high: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/high/'))))\n",
        "print('Number of samples with gleason grade low: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/low/'))))\n",
        "# applying class weights during model training\n",
        "neg = len(os.listdir('/content/data_lab/data_lab/train/low/'))\n",
        "pos = len(os.listdir('/content/data_lab/data_lab/train/high/'))\n",
        "total = neg + pos\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class low: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class high: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "id": "v5oJ4SCe8WGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will introduce class weights during the model training so the it takes into account the ratio of class imbalance while calculating the evaluation metric (i.e. loss) during the model training"
      ],
      "metadata": {
        "id": "gkutLiTQ8dE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=25,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset,\n",
        "                    class_weight=class_weight)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "5C111Ps-8bHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}