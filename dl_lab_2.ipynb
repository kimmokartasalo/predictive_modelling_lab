{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will go through the different aspects of common deep learning based problems, i.e. dealing with low dataset via transfer learning, class imbalance problem, overfitting and lastly hyperparameter optimisation"
      ],
      "metadata": {
        "id": "o7WmdloAg6km"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOV1EPP0dygG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()),1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(dpi=150)\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "            lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FgzzApded2A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and preparing the dataset\n",
        "\n",
        "!wget --load-cookies /tmp/data_lab.zip \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/data_lab.zip --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1xQEYahKo2RQIo8MW9MVHh7EPru8YFuV1\" -O data_lab.zip && rm -rf /tmp/data_lab.zip\n",
        "# if the first line doesn't work then uncomment and use the second one\n",
        "# !wget -O /content/data_lab.zip https://www.dropbox.com/scl/fi/meovuq3l3lkrhf2z0mq61/data_lab.zip?rlkey=4t1mec88ih21pyti83m5w2931&dl=0\n",
        "print('Unzipping folder... this may take a few minutes without output. Be patient.')\n",
        "with zipfile.ZipFile('data_lab.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_lab')\n",
        "\n",
        "path_data = '/content/data_lab/data_lab'\n",
        "train_dir = os.path.join(path_data, 'train')\n",
        "validation_dir = os.path.join(path_data, 'valid')\n",
        "test_dir = os.path.join(path_data, 'test')\n",
        "print('Directories in unzipped folder:', os.listdir(path_data))\n",
        "\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=16,\n",
        "                                             image_size=(598, 598))\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=16,\n",
        "                                                  image_size=(598, 598))\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=16,\n",
        "                                            image_size=(598, 598))\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "qGsD59-hq9ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class labels name\n",
        "print(class_names)\n"
      ],
      "metadata": {
        "id": "VIh38k1y9S5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = False\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=25,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "LIYXLXps2lEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe the model did not learn anything with random initalisation of the weights. A lot of data is required to fine tune the millions of parameters (Note: here we only fine tune the last 20% of the layers of the network so we did not optimise anything in the 80% of the network).\n",
        "\n",
        "Try running the model fitting with transfer learning where the pretrained model fitted on the Imagenet dataset is used to fine tune on the pathology images (same as the lab 1)"
      ],
      "metadata": {
        "id": "auG81SDMTKxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = True\n",
        "\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=25,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "supgagG3UXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What changes did you observe in the model learning curves?"
      ],
      "metadata": {
        "id": "xFV2Sw2lR9P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will include and observe the model overfitting and common strategies to deal with it.\n",
        "Strategies: including the data augmentation in the input, the dropout layer in the base model and the early stopping callback in the model training"
      ],
      "metadata": {
        "id": "g9Aj9d1kVpXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reducing the training dataset by a huge margin to introduce the overfitting\n",
        "train_dataset_subset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=16,\n",
        "                                             image_size=(598, 598),\n",
        "                                             validation_split = 0.99,\n",
        "                                             seed = 523,\n",
        "                                             subset = 'training')\n",
        "train_dataset_subset = train_dataset_subset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()"
      ],
      "metadata": {
        "id": "UNKrUxkMP49m",
        "outputId": "3dd3cde1-8128-49a1-9836-7014e463007f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10927 files belonging to 2 classes.\n",
            "Using 110 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = True\n",
        "\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "# steps_per_epoch = len(list(train_dataset_subset))\n",
        "# print(len(list(train_dataset_subset)))\n",
        "steps_per_epoch = int(110/16)\n",
        "history = model.fit(train_dataset_subset,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=25,\n",
        "                    epochs=30,\n",
        "                    validation_data=validation_dataset)\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "_0do11Ytudoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing the strategies to deal with overfitting"
      ],
      "metadata": {
        "id": "FZj-gj5O5zsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining data augmentation, potentially can add more different ones\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Create the base model from the pre-trained model or with randomly initialised weights in MobileNet V2 architecture\n",
        "transfer_learning = True\n",
        "if transfer_learning:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "else:\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=None,\n",
        "                                                 include_top=False,\n",
        "                                                 weights=None)\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "inputs = tf.keras.Input(shape=(598, 598, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "## include the dropout layer\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune only last 20% of layers\n",
        "fine_tune_at = int(0.8*len(base_model.layers))\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "# introducing callbacks and earlystopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "steps_per_epoch = int(110/16)\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=25,\n",
        "                    epochs=30,\n",
        "                    validation_data=validation_dataset,\n",
        "                    callbacks=[callback],\n",
        "                    verbose=1)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "yTeTAXQqSWV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can observe the training stopped early due to earlystopping callback criteria which avoided the overfitting as we observed in the previous model learning curves.\n",
        "If time allows you can apply the strategies one by one to see their effect on the model learning curves individually. You can try different parameters for dropout layer (dropout rate), early stopping (different evaluation metric to monitor, patience value etc.)"
      ],
      "metadata": {
        "id": "_lnaE9HIWC0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last part of the lab includes the optimisation of the hyperparameter: learning rate"
      ],
      "metadata": {
        "id": "FSWibd0IXPDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## creating a grid of learning rates\n",
        "list_of_learning_rates = [0.000001, 0.00001, 0.0001]\n",
        "for learning_rate in list_of_learning_rates:\n",
        "\n",
        "  loss = tf.keras.losses.BinaryCrossentropy()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "  history = model.fit(train_dataset,\n",
        "                      steps_per_epoch=25,\n",
        "                      validation_steps=25,\n",
        "                      epochs=15,\n",
        "                      validation_data=validation_dataset,\n",
        "                      callbacks=[callback],\n",
        "                      verbose=1)\n",
        "  plot_history(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyBAoQis0fUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will have to wait for the three models to finish training. This might take few minutes.\n",
        "\n",
        "What do you think is the optimum choice for the learning based on the model learning curves?"
      ],
      "metadata": {
        "id": "bcI1hTQZj1dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## looking at the number of images from each class in the training data\n",
        "print('Number of samples with gleason grade high: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/high/'))))\n",
        "print('Number of samples with gleason grade low: {}'.format(len(os.listdir('/content/data_lab/data_lab/train/low/'))))\n",
        "# applying class weights during model training\n",
        "neg = len(os.listdir('/content/data_lab/data_lab/train/low/'))\n",
        "pos = len(os.listdir('/content/data_lab/data_lab/train/high/'))\n",
        "total = neg + pos\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class low: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class high: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "id": "v5oJ4SCe8WGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will introduce class weights during the model training so the it takes into account the ratio of class imbalance while calculating the evaluation metric (i.e. loss) during the model training"
      ],
      "metadata": {
        "id": "gkutLiTQ8dE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=25,\n",
        "                    validation_steps=25,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_dataset,\n",
        "                    class_weight=class_weight)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "5C111Ps-8bHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}